% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.

@book{clrsAlgorithms,
author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
title = {Introduction to Algorithms, Third Edition},
year = {2009},
isbn = {0262033844},
publisher = {The MIT Press},
edition = {3rd},
annote  = {This is a popular algorithms textbook which is well-cited. In particular, Part VI on graph algorithms will be of interest. Chapter 26 discusses flow networks and introduces commonly used notation. It formally describes the problem of obtaining a maximum flow and its equivalence to obtaining a minimum cut. The classical method of Ford and Fulkerson's algorithm for finding a maximum flow is described, and it includes several examples. Additional methods for obtaining a maximum flow, including the push-relabel method, are also described. The chapter notes include additional references to specific articles which may be helpful, such as those of historical interest (the article in which an algorithm was originally proposed) as well as state-of-the-art improvements (more recent articles to improve the approach).}
}

@inproceedings{10.1145/3704637.3734789,
author = {Hug, Sarah and McKay, Mark},
title = {Problematizing {AI} Literacy Access - Understanding Student {AI} Literacy from Student Voices},
year = {2025},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704637.3734789},
doi = {10.1145/3704637.3734789},
abstract = {Domestic undergraduate computer science students formally learn about machine learning and artificial intelligence in upper level undergraduate computing programs, yet they must navigate the lure of ChatGPT and other generative Artificial Intelligence tools that have been found to be somewhat accurate at completing early coding assignments. As AI tools proliferate, messaging about their use in academic settings are varied, and access to AI literacy is unknown. Through an investigation of interviews with Pell grant eligible college students at open access colleges, we address the following research questions: How do low-income undergraduate interview participants describe their uses of and attitudes regarding generative AI tool use for academic purposes? and What elements of AI digital literacy appear to be accessible to interview participants, based on their descriptive statements?},
booktitle = {Proceedings of the 2025 Conference on Research on Equitable and Sustained Participation in Engineering, Computing, and Technology},
pages = {339â€“342},
numpages = {4},
keywords = {AI tool use, Pell-Grant eligible students, digital divide, generative AI literacy, qualitative interview},
location = {Newark, NJ, USA},
series = {RESPECT 2025}
}


@Article{info16090766,
AUTHOR = {Vinayan Kozhipuram, Aparna and Shailendra, Samar and Kadel, Rajan},
TITLE = {Retrieval-Augmented Generation vs. Baseline LLMs: A Multi-Metric Evaluation for Knowledge-Intensive Content},
JOURNAL = {Information},
VOLUME = {16},
YEAR = {2025},
NUMBER = {9},
ARTICLE-NUMBER = {766},
URL = {https://www.mdpi.com/2078-2489/16/9/766},
ISSN = {2078-2489},
ABSTRACT = {(1) Background: The development of Generative Artificial Intelligence (GenAI) is transforming knowledge-intensive domains such as Education. However, Large Language Models (LLMs), which serve as the foundational components for GenAI tools, are trained on static datasets, often producing misleading, factually incorrect, or outdated responses. Our study explores the performance gains of Retrieval-Augmented LLMs over baseline LLMs while also identifying the trade-off opportunity between smaller-parameter LLMs augmented with user-specific data to larger parameter LLMs. (2) Methods: We experimented with four different LLMs, each with a different number of parameters, to generate outputs. These outputs were then evaluated across seven lexical and semantic metrics to identify performance trends in Retrieval-Augmented Generation (RAG)-Augmented LLMs and analyze the impact of parameter size on LLM performance. (3) Results and Discussions: We have synthesized 968 different combinations to identify this trend with the help of different LLM sizes/parameters: TinyLlama 1.1B, Mistral 7B, Llama 3.1 8B, and Llama 1 13 B. These studies were grouped into two themes: RAG-Augmented LLM percentage improvements to baseline LLMs and compelling trade-off possibilities of RAG-Augmented smaller-parameter LLMs to larger-parameter LLMs. Our experiments show that RAG-Augmented LLMs demonstrate high lexical and semantic scores relative to baseline LLMs. This offers RAG-Augmented LLMs as a compelling trade-off for reducing the number of parameters in LLMs and lowering overall resource demands. (4) Conclusions: The findings outline that by leveraging RAG-Augmented LLMs, smaller-parameter LLMs can perform better or equivalently to large-parameter LLMs, particularly demonstrating strong lexical improvements. They reduce the risks of hallucination and keep the output more contextualized, making them a better choice for knowledge-intensive content in academic and research sectors.},
DOI = {10.3390/info16090766},
ANNOTE = {This paper compared four LLMs (from 1.1 Billion parameters to 13 Billion parameters), to see if Retrieval Augmented Generation (RAG) helps smaller models catch up to big ones. The models used were TinlyLlama 1.1B, Mistral 7B, Llama 3.1 8B and Llama 1 13 B. RAG is an AI architecture that optimizes the performance of models by allowing them to access external knowledge bases, enabling them to generate responses based on real, up-to-date data rather than solely relying on their training data. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. The researchers used a "multi -metric" approach using seven lexical and semantic scores to evaluate the performance gains of RAG LLM's over baseline LLMs. This is a 2025 peer reviewed article from the Information Journal, published by MDPI. Since I am interested in Environmental Studies and Data Science, one area of research for me could be looking into how RAG models might be more sustainable or cheaper to run compared to massive models.}
}




  


