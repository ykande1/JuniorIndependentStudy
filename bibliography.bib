% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.

@book{clrsAlgorithms,
author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
title = {Introduction to Algorithms, Third Edition},
year = {2009},
isbn = {0262033844},
publisher = {The MIT Press},
edition = {3rd},
annote  = {This is a popular algorithms textbook which is well-cited. In particular, Part VI on graph algorithms will be of interest. Chapter 26 discusses flow networks and introduces commonly used notation. It formally describes the problem of obtaining a maximum flow and its equivalence to obtaining a minimum cut. The classical method of Ford and Fulkerson's algorithm for finding a maximum flow is described, and it includes several examples. Additional methods for obtaining a maximum flow, including the push-relabel method, are also described. The chapter notes include additional references to specific articles which may be helpful, such as those of historical interest (the article in which an algorithm was originally proposed) as well as state-of-the-art improvements (more recent articles to improve the approach).}
}

@inproceedings{10.1145/3704637.3734789,
author = {Hug, Sarah and McKay, Mark},
title = {Problematizing {AI} Literacy Access - Understanding Student {AI} Literacy from Student Voices},
year = {2025},
isbn = {9798400706264},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704637.3734789},
doi = {10.1145/3704637.3734789},
abstract = {Domestic undergraduate computer science students formally learn about machine learning and artificial intelligence in upper level undergraduate computing programs, yet they must navigate the lure of ChatGPT and other generative Artificial Intelligence tools that have been found to be somewhat accurate at completing early coding assignments. As AI tools proliferate, messaging about their use in academic settings are varied, and access to AI literacy is unknown. Through an investigation of interviews with Pell grant eligible college students at open access colleges, we address the following research questions: How do low-income undergraduate interview participants describe their uses of and attitudes regarding generative AI tool use for academic purposes? and What elements of AI digital literacy appear to be accessible to interview participants, based on their descriptive statements?},
booktitle = {Proceedings of the 2025 Conference on Research on Equitable and Sustained Participation in Engineering, Computing, and Technology},
pages = {339–342},
numpages = {4},
keywords = {AI tool use, Pell-Grant eligible students, digital divide, generative AI literacy, qualitative interview},
location = {Newark, NJ, USA},
series = {RESPECT 2025}
}


@Article{info16090766,
AUTHOR = {Vinayan Kozhipuram, Aparna and Shailendra, Samar and Kadel, Rajan},
TITLE = {Retrieval-Augmented Generation vs. Baseline LLMs: A Multi-Metric Evaluation for Knowledge-Intensive Content},
JOURNAL = {Information},
VOLUME = {16},
YEAR = {2025},
NUMBER = {9},
ARTICLE-NUMBER = {766},
URL = {https://www.mdpi.com/2078-2489/16/9/766},
ISSN = {2078-2489},
ABSTRACT = {(1) Background: The development of Generative Artificial Intelligence (GenAI) is transforming knowledge-intensive domains such as Education. However, Large Language Models (LLMs), which serve as the foundational components for GenAI tools, are trained on static datasets, often producing misleading, factually incorrect, or outdated responses. Our study explores the performance gains of Retrieval-Augmented LLMs over baseline LLMs while also identifying the trade-off opportunity between smaller-parameter LLMs augmented with user-specific data to larger parameter LLMs. (2) Methods: We experimented with four different LLMs, each with a different number of parameters, to generate outputs. These outputs were then evaluated across seven lexical and semantic metrics to identify performance trends in Retrieval-Augmented Generation (RAG)-Augmented LLMs and analyze the impact of parameter size on LLM performance. (3) Results and Discussions: We have synthesized 968 different combinations to identify this trend with the help of different LLM sizes/parameters: TinyLlama 1.1B, Mistral 7B, Llama 3.1 8B, and Llama 1 13 B. These studies were grouped into two themes: RAG-Augmented LLM percentage improvements to baseline LLMs and compelling trade-off possibilities of RAG-Augmented smaller-parameter LLMs to larger-parameter LLMs. Our experiments show that RAG-Augmented LLMs demonstrate high lexical and semantic scores relative to baseline LLMs. This offers RAG-Augmented LLMs as a compelling trade-off for reducing the number of parameters in LLMs and lowering overall resource demands. (4) Conclusions: The findings outline that by leveraging RAG-Augmented LLMs, smaller-parameter LLMs can perform better or equivalently to large-parameter LLMs, particularly demonstrating strong lexical improvements. They reduce the risks of hallucination and keep the output more contextualized, making them a better choice for knowledge-intensive content in academic and research sectors.},
DOI = {10.3390/info16090766},
ANNOTE = {This paper compared four LLMs (from 1.1 Billion parameters to 13 Billion parameters), to see if Retrieval Augmented Generation (RAG) helps smaller models catch up to big ones. The models used were TinlyLlama 1.1B, Mistral 7B, Llama 3.1 8B and Llama 1 13 B. RAG is an AI architecture that optimizes the performance of models by allowing them to access external knowledge bases, enabling them to generate responses based on real, up-to-date data rather than solely relying on their training data. Unlike LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. The researchers used a "multi -metric" approach using seven lexical and semantic scores to evaluate the performance gains of RAG LLM's over baseline LLMs. This is a 2025 peer reviewed article from the Information Journal, published by MDPI. Since I am interested in Environmental Studies and Data Science, one area of research for me could be looking into how RAG models might be more sustainable or cheaper to run compared to massive models.}
}



@Article{technologies13100477,
AUTHOR = {Alevizos, Vasileios and Gerolimos, Nikitas and Leligkou, Eleni Aikaterini and Hompis, Giorgos and Priniotakis, Georgios and Papakostas, George A.},
TITLE = {Sustainable Swarm Intelligence: Assessing Carbon-Aware Optimization in High-Performance {AI} Systems},
JOURNAL = {Technologies},
VOLUME = {13},
YEAR = {2025},
NUMBER = {10},
ARTICLE-NUMBER = {477},
URL = {https://www.mdpi.com/2227-7080/13/10/477},
ISSN = {2227-7080},
ABSTRACT = {Carbon-aware AI demands clear links between algorithmic choices and verified emission outcomes. This study measures and steers the carbon footprint of swarm-based optimization in HPC by coupling a job-level Emission Impact Metric with sub-minute power and grid-intensity telemetry. Across 480 runs covering 41 algorithms, we report grams CO2 per successful optimisation and an efficiency index η (objective gain per kg CO2). Results show faster swarms achieve lower integral energy: Particle Swarm emits 24.9 g CO2 per optimum versus 61.3 g for GridSearch on identical hardware; Whale and Cuckoo approach the best η frontier, while L-SHADE exhibits front-loaded power spikes. Conservative scale factor schedules and moderate populations reduce emissions without degrading fitness; idle-node suppression further cuts leakage. Agreement between CodeCarbon, MLCO2, and vendor telemetry is within 1.8%, supporting reproducibility. The framework offers auditable, runtime controls (throttle/hold/release) that embed carbon objectives without violating solution quality budgets.},
DOI = {10.3390/technologies13100477},
ANNOTE = {This 2025 peer-reviewed article from the journal Technologies evaluates the carbon footprint of swarm-based optimization algorithms within High-Performance Computing (HPC) environments. The authors introduce a "Emission Impact Metric" that links algorithmic behavior to real-time power consumption and grid-intensity data. By analyzing 41 different algorithms across 480 experimental runs, the study highlights a significant efficiency gap; for instance, Particle Swarm Optimization emitted only 24.9 g of $CO_2$ per optimum compared to 61.3 g for standard Grid Search. The researchers propose a framework for auditable runtime controls that can throttle or release AI processes based on carbon objectives without sacrificing the quality of the solution. Seeing students working with Swarm Intelligence concepts for their Independent Study at Wooster got me thinkning about how I could apply these concepts to something I'm passionate about. I think this is one such study.}
}

@article{NGUYEN2021102947,
title = {Swarm intelligence-based green optimization framework for sustainable transportation},
journal = {Sustainable Cities and Society},
volume = {71},
pages = {102947},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102947},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721002328},
author = {Tri-Hai Nguyen and Jason J. Jung},
keywords = {Congestion mitigation, Connected vehicle, Green optimization, Repelling pheromone, Sustainable transportation, Swarm intelligence},
abstract = {Traffic congestion is one of the most critical issues in developing sustainable transportation in smart cities. As the Internet of Things evolves, connected vehicle technology has arisen as an essential research topic in smart, sustainable transportation. This study investigates a decentralized green traffic optimization framework by pushing swarm intelligence into connected vehicles to mitigate traffic congestion. We present a dynamic traffic routing method based on ant species’ swarm intelligence for connected vehicles so that they can communicate with each other and their surrounding environment via digital pheromones to perform routing decision-making in a decentralized manner. Traditional pheromones attract other vehicles to move to the optimal path, which will soon be congested if many vehicles travel on that path concurrently. To overcome this limitation, we propose the concept of repelling pheromone, which generates a repulsive force among vehicles so that their travel paths are distributed throughout a road network, resulting in a congestion-free road network. The proposed method is validated in the Simulation of Urban Mobility platform. Simulation findings reveal that the proposed method outperforms baseline methods in mitigating traffic congestion, reducing average fuel consumption and emissions by 13–19\% and the average trip duration by 19–28\%.},
annote = {This 2021 peer-reviewed article from the journal Sustainable Cities and Society presents a decentralized green traffic optimization framework aimed at creating sustainable transportation systems within smart cities. The core innovation of the paper is the concept of a "repelling pheromone," which, unlike traditional pheromones that attract agents to a single path, generates a repulsive force to distribute vehicle travel paths across a road network. Validated through the Simulation of Urban Mobility (SUMO) platform, the study demonstrates significant environmental benefits, including a 13–19\% reduction in fuel consumption and emissions.
This study provides a technical roadmap for how swarm-based IoT systems can directly mitigate the "Tragedy of the Commons" in urban infrastructure, which is a highly relevant issue to address.}
}

@article{article,
author = {Arner, Douglas and Barberis, Janos and Buckley, Ross},
year = {2017},
month = {01},
pages = {373-415},
title = {FinTech, regTech, and the reconceptualization of financial regulation},
volume = {37},
journal = {Northwestern Journal of International Law and Business}
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@misc{shuster2021retrievalaugmentationreduceshallucination,
      title={Retrieval Augmentation Reduces Hallucination in Conversation}, 
      author={Kurt Shuster and Spencer Poff and Moya Chen and Douwe Kiela and Jason Weston},
      year={2021},
      eprint={2104.07567},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.07567}, 
}




  


